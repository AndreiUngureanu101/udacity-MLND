{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "# Import dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "%matplotlib qt\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "from mne import pick_types, Annotations, create_info, find_events, Epochs\n",
    "from mne.channels import make_standard_montage\n",
    "from mne.io import RawArray\n",
    "from mne.epochs import concatenate_epochs\n",
    "from mne.decoding import CSP\n",
    "\n",
    "from scipy.signal import welch\n",
    "from mne.viz.topomap import _prepare_topo_plot, plot_topomap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import pickle as pkl\n",
    "\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a neural network\n",
    "\n",
    "###### We'll use series 1 through 6 for training and series 7 and 8 for validating the deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data file names for a all subject, series 1 through 8 for normalization\n",
    "series = list(range(1,9))\n",
    "file = f'grasp-and-lift-eeg-detection/train/subj*_series{series}_data.csv'\n",
    "dirpath = os.path.join(os.getcwd(),file)\n",
    "glob_object = glob.glob(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the data and calculate the mean and standard deviation for each channel\n",
    "data_all = np.empty((32,0))\n",
    "for file in glob_object:\n",
    "    data_df = pd.read_csv(file).drop(['id'], axis=1)\n",
    "    data_np = data_df.to_numpy().transpose()\n",
    "    data_all = np.concatenate((data_all,data_np),axis=1)\n",
    "MEAN = data_all.mean(1).reshape(32,1)\n",
    "ST_DEV = data_all.std(1).reshape(32,1)\n",
    "maximum = np.amax(data_all, axis = 1).reshape(32,1)\n",
    "minimum = np.amin(data_all, axis = 1).reshape(32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(MEAN, open(\"MEAN.pkl\", \"wb\"))\n",
    "pkl.dump(ST_DEV, open(\"ST_DEV.pkl\", \"wb\"))\n",
    "pkl.dump(maximum, open(\"maximum.pkl\", \"wb\"))\n",
    "pkl.dump(minimum, open(\"minimum.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = pkl.load( open( \"MEAN.pkl\", \"rb\" ) )\n",
    "ST_DEV = pkl.load( open( \"ST_DEV.pkl\", \"rb\" ) )\n",
    "maximum = pkl.load( open( \"maximum.pkl\", \"rb\" ) )\n",
    "minimum = pkl.load( open( \"minimum.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data file names for a all subject, series 1 through 6 for generating the training data\n",
    "series = list(range(1,7))\n",
    "file = f'grasp-and-lift-eeg-detection/train/subj*_series{series}_data.csv'\n",
    "dirpath = os.path.join(os.getcwd(),file)\n",
    "glob_object = glob.glob(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "win_length = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(data,events,batch_size,win_length):\n",
    "    batches = data.shape[1]//(win_length*batch_size)  \n",
    "    \n",
    "    data_out = np.empty((batches,batch_size,32,win_length))\n",
    "    events_out = np.empty((batches,batch_size,6))\n",
    "    \n",
    "    data_temp = np.empty((batch_size,32,win_length))\n",
    "    events_temp = np.empty((batch_size,6))\n",
    "    \n",
    "    batch_length = batch_size*win_length\n",
    "    \n",
    "    for k in range(batches):\n",
    "        for i in range(batch_size):\n",
    "            data_temp[i,:,:] = data[:,(k*batch_length)+(i*win_length):(k*batch_length)+(i+1)*win_length]\n",
    "            events_temp[i] = events[:,(k*batch_length)+(i+1)*win_length-1]\n",
    "        data_out[k,:,:,:] = data_temp\n",
    "        events_out[k,:,:] = events_temp\n",
    "    return (data_out, events_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(glob_object):\n",
    "    \n",
    "    X = np.empty((0,batch_size,32,win_length))\n",
    "    Y = np.empty((0,batch_size,6))\n",
    "\n",
    "    for file in glob_object:\n",
    "\n",
    "        data_df = pd.read_csv(file).drop(['id'], axis=1)\n",
    "        data_np = data_df.to_numpy().transpose()\n",
    "        # threshold data so it can fit in batches\n",
    "        thresh = data_np.shape[1] - (data_np.shape[1] % (batch_size*win_length))\n",
    "        data_np = data_np[:,:thresh]\n",
    "        #normalize the data\n",
    "        data_np = data_np - minimum\n",
    "        data_np = data_np  / (maximum - minimum)     \n",
    "        file = file.replace('data','events')\n",
    "        events_df = pd.read_csv(file).drop(['id'], axis=1)\n",
    "        events_np = events_df.to_numpy().transpose()\n",
    "        events_np = events_np[:,:thresh]   \n",
    "\n",
    "        data, events = reshape_data(data_np,events_np,batch_size,win_length)\n",
    "\n",
    "        X = np.concatenate((X,data),axis=0)\n",
    "        Y = np.concatenate((Y,events),axis=0)\n",
    "\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = generate_data(glob_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3046, 32, 32, 150)\n",
      "(3046, 32, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.amax(X_train[:,:,:,:]))\n",
    "print(np.amin(X_train[:,:,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(X_train, open(\"X_train.pkl\", \"wb\"))\n",
    "pkl.dump(Y_train, open(\"Y_train.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pkl.load( open( \"X_train.pkl\", \"rb\" ) )\n",
    "Y_train = pkl.load( open( \"Y_train.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data file names for a all subject, series 7 and 8 for building the validation data\n",
    "series = [7,8]\n",
    "file = f'grasp-and-lift-eeg-detection/train/subj*_series{series}_data.csv'\n",
    "dirpath = os.path.join(os.getcwd(),file)\n",
    "glob_object = glob.glob(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, Y_valid = generate_data(glob_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(646, 32, 32, 150)\n",
      "(646, 32, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_valid.shape)\n",
    "print(Y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.amax(X_valid[:,:,:,:]))\n",
    "print(np.amin(X_valid[:,:,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(X_valid, open(\"X_valid.pkl\", \"wb\"))\n",
    "pkl.dump(Y_valid, open(\"Y_valid.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = pkl.load( open( \"X_valid.pkl\", \"rb\" ) )\n",
    "Y_valid = pkl.load( open( \"Y_valid.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_result(arr, batch_size):\n",
    "    return torch.where(arr > 0.5, torch.ones(batch_size,6), torch.zeros(batch_size,6)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(model,X,Y):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for i in range(X.shape[0]):\n",
    "        y_hat = model(torch.tensor(X[i,:,:,:]))\n",
    "        y_hat = binarize_result(y_hat,batch_size)\n",
    "        y = Y[i,:,:]\n",
    "        for j in range(batch_size):\n",
    "            if (sum(y[j]) != 0):\n",
    "                total += 1\n",
    "            if (sum(y[j]) == 0 and sum(y_hat[j]) != 0 ):\n",
    "                fp += 1\n",
    "            if (sum(y[j]) != 0 and np.array_equal(y[j],y_hat[j])):\n",
    "                tp += 1   \n",
    "    print(\" tp:\", tp, \" fp:\", fp)\n",
    "    if (tp+fp) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return tp/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_valid_loss(model,X,Y):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for i in range(X.shape[0]):\n",
    "        y_hat = model(torch.tensor(X[i,:,:,:])).detach().numpy()\n",
    "        y = Y[i,:,:] \n",
    "        total_loss += log_loss(y,y_hat)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX 1070\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneDimensionalConvolution(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size):\n",
    "        super(OneDimensionalConvolution, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.c1 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, stride=1)\n",
    "        self.c2 = nn.Conv1d(in_channels=16, out_channels=8, kernel_size=3, stride=1)\n",
    "        self.m1 = nn.MaxPool1d(3, stride=2)     \n",
    "        \n",
    "        self.c3 = nn.Conv1d(in_channels=8, out_channels=32, kernel_size=3, stride=1)\n",
    "        self.c4 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, stride=1) \n",
    "        self.m2 = nn.MaxPool1d(3, stride=2)   \n",
    "        \n",
    "        self.c5 = nn.Conv1d(in_channels=16, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.c6 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, stride=1) \n",
    "        self.m3 = nn.MaxPool1d(3, stride=2)           \n",
    "        \n",
    "        self.c7 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.c8 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, stride=1) \n",
    "        self.m4 = nn.MaxPool1d(3, stride=2)    \n",
    "               \n",
    "        self.d1 = nn.Dropout(p=0.1)     \n",
    "        self.l1 = nn.Linear(128,64)\n",
    "        \n",
    "        self.d2 = nn.Dropout(p=0.1)\n",
    "        self.l2 = nn.Linear(64,64)\n",
    "        \n",
    "        self.l3 = nn.Linear(64,6)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = self.m1(self.c2(self.c1(x))) \n",
    "        x = self.m2(self.c4(self.c3(x)))\n",
    "        x = self.m3(self.c6(self.c5(x)))     \n",
    "        x = self.m4(self.c8(self.c7(x)))\n",
    "        \n",
    "        x = self.d1(x)\n",
    "        x = x.reshape(self.batch_size,-1)\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.d2(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.l3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneDimensionalConvolution(batch_size).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, running loss: 373.2571550890378, validation loss: 269.439486451792\n",
      "epoch: 1, running loss: 336.750709946037, validation loss: 269.4418428498164\n",
      "epoch: 2, running loss: 333.4784183917716, validation loss: 269.45405957277944\n",
      "epoch: 3, running loss: 330.2262143113399, validation loss: 269.42446083833534\n",
      "epoch: 4, running loss: 328.6226027112751, validation loss: 269.31415990670376\n",
      "epoch: 5, running loss: 327.93394052441874, validation loss: 268.87443769434157\n",
      "epoch: 6, running loss: 328.5576649757252, validation loss: 268.70299965778395\n",
      "epoch: 7, running loss: 328.581937702168, validation loss: 268.6350733686502\n",
      "epoch: 8, running loss: 329.40412451458053, validation loss: 268.68279488307115\n",
      "epoch: 9, running loss: 329.3682731771469, validation loss: 268.5310413987213\n",
      "epoch: 10, running loss: 329.57173835838347, validation loss: 268.4110086196021\n",
      "epoch: 11, running loss: 329.76095309934783, validation loss: 268.4262579462506\n",
      "epoch: 12, running loss: 329.39950277662734, validation loss: 268.483362961207\n",
      "epoch: 13, running loss: 329.065794597696, validation loss: 268.3518288594722\n",
      "epoch: 14, running loss: 328.8217265215961, validation loss: 268.2071350297895\n",
      "epoch: 15, running loss: 327.8911088059729, validation loss: 268.0042390039066\n",
      "epoch: 16, running loss: 326.64570625011805, validation loss: 267.6764378026989\n",
      "epoch: 17, running loss: 325.66794257979666, validation loss: 266.90739089741\n",
      "epoch: 18, running loss: 324.93234528683547, validation loss: 266.18236578284615\n",
      "epoch: 19, running loss: 324.2091642732108, validation loss: 264.3770892380784\n",
      "epoch: 20, running loss: 323.8240184285635, validation loss: 264.33316711756225\n",
      "epoch: 21, running loss: 323.4775796434132, validation loss: 264.0363769844274\n",
      "epoch: 22, running loss: 323.15190780997744, validation loss: 263.8195221972667\n",
      "epoch: 23, running loss: 322.87029555227394, validation loss: 263.4766914592878\n",
      "epoch: 24, running loss: 322.6149714525114, validation loss: 263.8031104862838\n",
      "epoch: 25, running loss: 322.2418306646255, validation loss: 262.9821111832677\n",
      "epoch: 26, running loss: 322.02842012765507, validation loss: 263.1392068875224\n",
      "epoch: 27, running loss: 321.85144345284584, validation loss: 263.6950588204784\n",
      "epoch: 28, running loss: 321.7172703514499, validation loss: 263.053336104095\n",
      "epoch: 29, running loss: 321.1889099808077, validation loss: 262.56973406519097\n",
      "epoch: 30, running loss: 320.92369323605186, validation loss: 263.1924617939446\n",
      "epoch: 31, running loss: 320.7642394229044, validation loss: 263.1199066866257\n",
      "epoch: 32, running loss: 320.5631693509228, validation loss: 262.89301235215436\n",
      "epoch: 33, running loss: 320.33421931487806, validation loss: 262.2525967718287\n",
      "epoch: 34, running loss: 320.0687559555616, validation loss: 261.93431719454753\n",
      "epoch: 35, running loss: 319.8549624794269, validation loss: 262.0343095879209\n",
      "epoch: 36, running loss: 319.43297012867697, validation loss: 261.120905821057\n",
      "epoch: 37, running loss: 319.10561109066566, validation loss: 261.6127414872728\n",
      "epoch: 38, running loss: 319.0545781463545, validation loss: 260.77632430823434\n",
      "epoch: 39, running loss: 318.6358615279539, validation loss: 260.47469074010036\n",
      "epoch: 40, running loss: 318.5961329916086, validation loss: 260.4908975164451\n",
      "epoch: 41, running loss: 318.073619551099, validation loss: 261.4741415238656\n",
      "epoch: 42, running loss: 318.04596190415344, validation loss: 259.3966512290487\n",
      "epoch: 43, running loss: 317.794660132864, validation loss: 261.5010576418447\n",
      "epoch: 44, running loss: 317.7212035430078, validation loss: 260.8086443554558\n",
      "epoch: 45, running loss: 317.02227182766285, validation loss: 263.5331935003541\n",
      "epoch: 46, running loss: 317.2356248517349, validation loss: 262.00242896457667\n",
      "epoch: 47, running loss: 316.41945968231613, validation loss: 262.72768835569303\n",
      "epoch: 48, running loss: 316.4614589899253, validation loss: 262.0902327622364\n",
      "epoch: 49, running loss: 316.2266151121643, validation loss: 262.74130855417866\n",
      "epoch: 50, running loss: 316.06296654395555, validation loss: 261.9722079057472\n",
      "epoch: 51, running loss: 315.6993629832997, validation loss: 263.1495315578922\n",
      "epoch: 52, running loss: 315.5778471714594, validation loss: 264.1845168814319\n",
      "epoch: 53, running loss: 315.49734135841686, validation loss: 262.1555263140231\n",
      "epoch: 54, running loss: 315.0857565433113, validation loss: 264.90239919601254\n",
      "epoch: 55, running loss: 314.9926847155644, validation loss: 262.9220371384141\n",
      "epoch: 56, running loss: 314.50665551382053, validation loss: 261.6351221643692\n",
      "epoch: 57, running loss: 314.39371622390127, validation loss: 260.03394819408106\n",
      "epoch: 58, running loss: 314.362007455836, validation loss: 264.0712552106232\n",
      "epoch: 59, running loss: 314.1267888696378, validation loss: 262.0394508587601\n",
      "epoch: 60, running loss: 313.98720345314825, validation loss: 259.2591578344173\n",
      "epoch: 61, running loss: 313.8588116804861, validation loss: 259.6718607846015\n",
      "epoch: 62, running loss: 313.5381921581222, validation loss: 263.23096134252376\n",
      "epoch: 63, running loss: 313.45073437750125, validation loss: 263.47591762413475\n",
      "epoch: 64, running loss: 313.01029505700546, validation loss: 263.57921222726054\n",
      "epoch: 65, running loss: 312.7257508189111, validation loss: 259.8737129042108\n",
      "epoch: 66, running loss: 312.48725527814935, validation loss: 260.4478004049301\n",
      "epoch: 67, running loss: 312.45868578566893, validation loss: 261.5549323654178\n",
      "epoch: 68, running loss: 312.09563880982364, validation loss: 262.21994072408285\n",
      "epoch: 69, running loss: 311.56258972810946, validation loss: 259.88660925166204\n",
      "epoch: 70, running loss: 311.259557009539, validation loss: 261.79897701638635\n",
      "epoch: 71, running loss: 311.1660186434334, validation loss: 261.3427790448867\n",
      "epoch: 72, running loss: 310.69229227313474, validation loss: 261.3174692530906\n",
      "epoch: 73, running loss: 310.59578727036666, validation loss: 261.4271119467844\n",
      "epoch: 74, running loss: 310.24384261199054, validation loss: 261.52938008392897\n",
      "epoch: 75, running loss: 309.73562459367923, validation loss: 260.2258788038215\n",
      "epoch: 76, running loss: 309.56919316935466, validation loss: 259.57576088527037\n",
      "epoch: 77, running loss: 309.21474944436795, validation loss: 258.36408222797775\n",
      "epoch: 78, running loss: 308.8785257264696, validation loss: 260.4434606373076\n",
      "epoch: 79, running loss: 308.38693059509103, validation loss: 264.4032502012163\n",
      "epoch: 80, running loss: 308.3706026214044, validation loss: 259.8294874952083\n",
      "epoch: 81, running loss: 308.3516940350131, validation loss: 263.20512445932167\n",
      "epoch: 82, running loss: 307.8618356437704, validation loss: 260.4106094126998\n",
      "epoch: 83, running loss: 307.3114424304172, validation loss: 262.9839299196036\n",
      "epoch: 84, running loss: 306.95345768275865, validation loss: 261.75681772923974\n",
      "epoch: 85, running loss: 306.9763660433023, validation loss: 261.68461139826894\n",
      "epoch: 86, running loss: 306.502951738034, validation loss: 262.6704783354042\n",
      "epoch: 87, running loss: 306.6126144736275, validation loss: 260.84606792880186\n",
      "epoch: 88, running loss: 306.1647056160969, validation loss: 264.0301292191398\n",
      "epoch: 89, running loss: 306.1277714097249, validation loss: 265.34665372070447\n",
      "epoch: 90, running loss: 305.6320114548559, validation loss: 264.3278335745903\n",
      "epoch: 91, running loss: 305.50064401001777, validation loss: 263.6725120025719\n",
      "epoch: 92, running loss: 304.99888666028517, validation loss: 263.1389975787341\n",
      "epoch: 93, running loss: 305.1225767207409, validation loss: 263.42429701451533\n",
      "epoch: 94, running loss: 304.8713288835232, validation loss: 263.5704175847971\n",
      "epoch: 95, running loss: 304.7010369146225, validation loss: 260.00776346594233\n",
      "epoch: 96, running loss: 304.1124497451219, validation loss: 262.7462841793098\n",
      "epoch: 97, running loss: 304.10099585051637, validation loss: 264.063886979903\n",
      "epoch: 98, running loss: 303.73536896378977, validation loss: 263.86307730697644\n",
      "epoch: 99, running loss: 303.58500706449246, validation loss: 266.36122314091244\n",
      "epoch: 100, running loss: 303.1685516936547, validation loss: 263.6666830705718\n",
      "epoch: 101, running loss: 303.1027204374491, validation loss: 268.08183144421895\n",
      "epoch: 102, running loss: 303.12455653828846, validation loss: 262.3326780596979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 103, running loss: 302.59131882575207, validation loss: 264.3094536753783\n",
      "epoch: 104, running loss: 302.3143236372269, validation loss: 267.2330985103897\n",
      "epoch: 105, running loss: 302.22362041312954, validation loss: 263.71319850798335\n",
      "epoch: 106, running loss: 301.58415382313706, validation loss: 262.64733416902203\n",
      "epoch: 107, running loss: 301.5944789181581, validation loss: 265.3894219398555\n",
      "epoch: 108, running loss: 301.54662857022475, validation loss: 264.1318263859532\n",
      "epoch: 109, running loss: 301.12003179433657, validation loss: 264.565037171427\n",
      "epoch: 110, running loss: 300.91130707537286, validation loss: 264.89488354467375\n",
      "epoch: 111, running loss: 300.89619806925725, validation loss: 268.84176667612326\n",
      "epoch: 112, running loss: 300.56240408608693, validation loss: 261.9750501552991\n",
      "epoch: 113, running loss: 300.17289681839395, validation loss: 267.52475031334416\n",
      "epoch: 114, running loss: 299.8006638536248, validation loss: 263.13897181639186\n",
      "epoch: 115, running loss: 299.3715514070523, validation loss: 265.5853765075951\n",
      "epoch: 116, running loss: 299.58967479621236, validation loss: 265.24721483577076\n",
      "epoch: 117, running loss: 299.02638759601297, validation loss: 266.54304743246007\n",
      "epoch: 118, running loss: 298.82632317958405, validation loss: 268.1076761570868\n",
      "epoch: 119, running loss: 298.5196668893088, validation loss: 267.81681797533554\n",
      "epoch: 120, running loss: 298.43557001068916, validation loss: 269.01327327764875\n",
      "epoch: 121, running loss: 298.33281431762754, validation loss: 264.6115782608857\n",
      "epoch: 122, running loss: 297.8108748316432, validation loss: 268.90227416402615\n",
      "epoch: 123, running loss: 297.49823415587394, validation loss: 271.63685056308714\n",
      "epoch: 124, running loss: 297.0367912121809, validation loss: 269.8763702400217\n",
      "epoch: 125, running loss: 296.54332790104536, validation loss: 273.9419369391059\n",
      "epoch: 126, running loss: 296.5944958774863, validation loss: 266.0332678210478\n",
      "epoch: 127, running loss: 296.5343786256273, validation loss: 268.6230828445338\n",
      "epoch: 128, running loss: 296.1708306899768, validation loss: 268.5484029000856\n",
      "epoch: 129, running loss: 296.0027973977555, validation loss: 272.05484126962324\n",
      "epoch: 130, running loss: 295.27050509403745, validation loss: 275.17386507596115\n",
      "epoch: 131, running loss: 295.008889658292, validation loss: 273.6003499253808\n",
      "epoch: 132, running loss: 295.34303591474526, validation loss: 271.8687192626624\n",
      "epoch: 133, running loss: 294.57566298937934, validation loss: 275.67866647154597\n",
      "epoch: 134, running loss: 294.51326102175807, validation loss: 270.6844823140552\n",
      "epoch: 135, running loss: 294.6616153254644, validation loss: 269.9003126910364\n",
      "epoch: 136, running loss: 293.4068715940594, validation loss: 268.4218190880655\n",
      "epoch: 137, running loss: 293.46016556514536, validation loss: 268.7937757647408\n",
      "epoch: 138, running loss: 293.3356607129524, validation loss: 270.1699428482309\n",
      "epoch: 139, running loss: 292.87089225095923, validation loss: 271.37427692676346\n",
      "epoch: 140, running loss: 292.4807995951245, validation loss: 274.56624517520714\n",
      "epoch: 141, running loss: 292.0383988443872, validation loss: 283.7248016731489\n",
      "epoch: 142, running loss: 291.5091085094323, validation loss: 281.87002242091984\n",
      "epoch: 143, running loss: 291.3939892800376, validation loss: 278.21734840967963\n",
      "epoch: 144, running loss: 290.7346724145176, validation loss: 279.6238469554372\n",
      "epoch: 145, running loss: 291.4939282354754, validation loss: 278.3502182267072\n",
      "epoch: 146, running loss: 290.29419790384065, validation loss: 276.2057782863939\n",
      "epoch: 147, running loss: 290.105237288136, validation loss: 276.5694026934153\n",
      "epoch: 148, running loss: 289.35212476612213, validation loss: 280.7081902277626\n",
      "epoch: 149, running loss: 289.18481548101505, validation loss: 278.78147407133054\n",
      "epoch: 150, running loss: 288.99988785241106, validation loss: 277.1720678536404\n",
      "epoch: 151, running loss: 288.3961679896933, validation loss: 276.44201309773626\n",
      "epoch: 152, running loss: 288.4847180619705, validation loss: 275.28082351093195\n",
      "epoch: 153, running loss: 287.89999516827083, validation loss: 273.9917377230397\n",
      "epoch: 154, running loss: 287.6576926350684, validation loss: 275.6941924993057\n",
      "epoch: 155, running loss: 286.78348484859885, validation loss: 287.13073750971574\n",
      "epoch: 156, running loss: 287.3100997750699, validation loss: 279.04479171211887\n",
      "epoch: 157, running loss: 286.3666681303649, validation loss: 283.591516965047\n",
      "epoch: 158, running loss: 285.917479304362, validation loss: 278.53749796006656\n",
      "epoch: 159, running loss: 285.371327819372, validation loss: 286.8587963495135\n",
      "epoch: 160, running loss: 285.59237286506794, validation loss: 278.54894288097125\n",
      "epoch: 161, running loss: 285.7396060406205, validation loss: 282.66653962904115\n",
      "epoch: 162, running loss: 284.45732345238076, validation loss: 282.44397549846957\n",
      "epoch: 163, running loss: 284.0192074917231, validation loss: 285.7519981554127\n",
      "epoch: 164, running loss: 284.06746692320456, validation loss: 288.21947785898845\n",
      "epoch: 165, running loss: 284.40337749836425, validation loss: 282.93830441733706\n",
      "epoch: 166, running loss: 282.515374544721, validation loss: 290.5385529934991\n",
      "epoch: 167, running loss: 282.9683375450445, validation loss: 289.2395318747533\n",
      "epoch: 168, running loss: 282.5403238062172, validation loss: 284.0983710993632\n",
      "epoch: 169, running loss: 282.35346751768157, validation loss: 291.393696642197\n",
      "epoch: 170, running loss: 281.093341692443, validation loss: 286.0058036164443\n",
      "epoch: 171, running loss: 280.932006903138, validation loss: 279.0436142935724\n",
      "epoch: 172, running loss: 281.2144807297171, validation loss: 285.7039484658291\n",
      "epoch: 173, running loss: 280.67510016937763, validation loss: 282.8893984167264\n",
      "epoch: 174, running loss: 280.1338480377127, validation loss: 291.8878848411595\n",
      "epoch: 175, running loss: 280.7082036537408, validation loss: 283.4766955963525\n",
      "epoch: 176, running loss: 279.690484220895, validation loss: 283.39758859129137\n",
      "epoch: 177, running loss: 279.2493521049402, validation loss: 286.2254587703777\n",
      "epoch: 178, running loss: 278.31542127290356, validation loss: 290.10671894093457\n",
      "epoch: 179, running loss: 278.04012943557456, validation loss: 291.9754842453303\n",
      "epoch: 180, running loss: 278.3432903813969, validation loss: 288.4431763464105\n",
      "epoch: 181, running loss: 277.6937697587092, validation loss: 280.92927724894616\n",
      "epoch: 182, running loss: 277.6146260681556, validation loss: 292.1330082315189\n",
      "epoch: 183, running loss: 277.2394003239156, validation loss: 287.2999298592066\n",
      "epoch: 184, running loss: 277.4886174209342, validation loss: 295.48361482059755\n",
      "epoch: 185, running loss: 275.6808225951018, validation loss: 295.63483511596957\n",
      "epoch: 186, running loss: 275.9989060114995, validation loss: 300.98632947095933\n",
      "epoch: 187, running loss: 276.37820311960684, validation loss: 290.29796188757183\n",
      "epoch: 188, running loss: 276.08358061439617, validation loss: 291.20403393246045\n",
      "epoch: 189, running loss: 275.12194703974336, validation loss: 294.20254501506156\n",
      "epoch: 190, running loss: 275.32025014925887, validation loss: 285.5269654040262\n",
      "epoch: 191, running loss: 274.84054815076905, validation loss: 298.3669456356797\n",
      "epoch: 192, running loss: 274.61204506209424, validation loss: 297.0072691621688\n",
      "epoch: 193, running loss: 273.54822989732725, validation loss: 298.6980576356263\n",
      "epoch: 194, running loss: 274.515422892574, validation loss: 294.1975132646926\n",
      "epoch: 195, running loss: 273.69510687734817, validation loss: 288.10202041928403\n",
      "epoch: 196, running loss: 272.6481120063433, validation loss: 298.2699116091412\n",
      "epoch: 197, running loss: 272.2966980693455, validation loss: 308.4012363957236\n",
      "epoch: 198, running loss: 272.538263597999, validation loss: 296.95307320217495\n",
      "epoch: 199, running loss: 273.1436065890313, validation loss: 304.2802557414856\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "no_batches = X_train.shape[0]\n",
    "\n",
    "for epoch in range(200):   \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for k in range(no_batches):\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        y = model(torch.tensor(X_train[k,:,:,:],requires_grad=True))\n",
    "        y_hat = torch.tensor(Y_train[k,:,:])\n",
    "        loss = criterion(y, y_hat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f'epoch: {epoch}, running loss: {running_loss}, validation loss: {calculate_valid_loss(model,X_valid,Y_valid)}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################################################### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = os.path.join(os.getcwd(),'model_SGD_minmax_16_2')\n",
    "torch.save(model.state_dict(), dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneDimensionalConvolution(batch_size).double()\n",
    "model.load_state_dict(torch.load(os.path.join(os.getcwd(),'model_SGD_5')))\n",
    "model.eval()\n",
    "calculate_precision(model, X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj1_series9_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj1_series10_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj2_series9_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj2_series10_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj3_series9_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj3_series10_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj4_series9_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj4_series10_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj5_series9_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj5_series10_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj6_series9_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj6_series10_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj7_series9_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj7_series10_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj8_series9_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj8_series10_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj9_series9_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj9_series10_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj10_series9_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj10_series10_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj11_series9_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj11_series10_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj12_series9_data.csv\n",
      "/home/cinetic-vr/EEG/grasp-and-lift-eeg-detection/test/subj12_series10_data.csv\n"
     ]
    }
   ],
   "source": [
    "# get the test data and arrange the data to have the same order as in the submission file\n",
    "file = f'grasp-and-lift-eeg-detection/test/subj*_series9_data.csv'\n",
    "dirpath = os.path.join(os.getcwd(),file)\n",
    "glob_object1 = glob.glob(dirpath)\n",
    "\n",
    "file = f'grasp-and-lift-eeg-detection/test/subj*_series10_data.csv'\n",
    "dirpath = os.path.join(os.getcwd(),file)\n",
    "glob_object2 = glob.glob(dirpath)\n",
    "\n",
    "glob_object1.sort()\n",
    "glob_object2.sort()\n",
    "\n",
    "glob_object1 = glob_object1[3:]+glob_object1[:3]\n",
    "glob_object2 = glob_object2[3:]+glob_object2[:3]\n",
    "\n",
    "glob_object = []\n",
    "\n",
    "for i in range(12):\n",
    "    glob_object.append(glob_object1[i])\n",
    "    glob_object.append(glob_object2[i])\n",
    "    \n",
    "for f in glob_object:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_test_data(data,batch_size,win_length):\n",
    "    batches = data.shape[1]//(win_length*batch_size)  \n",
    "    \n",
    "    data_out = np.empty((batches,batch_size,32,win_length))   \n",
    "    data_temp = np.empty((batch_size,32,win_length))\n",
    "    batch_length = batch_size*win_length\n",
    "    \n",
    "    for k in range(batches):\n",
    "        for i in range(batch_size):\n",
    "            data_temp[i,:,:] = data[:,(k*batch_length)+(i*win_length):(k*batch_length)+(i+1)*win_length]\n",
    "        data_out[k,:,:,:] = data_temp\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.empty((0,batch_size,32,win_length))\n",
    "\n",
    "for file in glob_object:\n",
    "\n",
    "    data_df = pd.read_csv(file).drop(['id'], axis=1)\n",
    "    data_np = data_df.to_numpy().transpose()\n",
    "    # threshold data so it can fit in batches\n",
    "    thresh = data_np.shape[1] - (data_np.shape[1] % (batch_size*win_length))\n",
    "    data_np = data_np[:,:thresh]\n",
    "    #normalize the data\n",
    "    data_np = (data_np - MEAN) / ST_DEV\n",
    "    data = reshape_test_data(data_np,batch_size,win_length)\n",
    "\n",
    "    X_test = np.concatenate((X_test,data),axis=0)\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(642, 32, 32, 150)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
